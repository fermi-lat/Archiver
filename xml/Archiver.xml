<?xml version="1.0" encoding="UTF-8"?>
<pipeline
  xmlns="http://glast-ground.slac.stanford.edu/pipeline"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://glast-ground.slac.stanford.edu/pipeline https://glast-ground.slac.stanford.edu/Pipeline-II/schemas/2.3/pipeline.xsd">

  <!-- Change log.
       v1.3 2020-03-10 tether First production version.
       v1.4 2020-04-07 tether Use the RHEL6 ISOC build in order
           to avoid a problem with missing OpenSSL 0.98 libs
           in the RHEL 5 build.
       v1.7 2020-04-08 Ensure that no bash script line begins with a tab
           or else the script will get errors due to command "11"
           not being found.
       v1.8 2020-04-15 Make sure that all dates and times are UTC.
       v2.0 2026-02-05 omodei first version to be used on S3DF
  -->

  <!-- The type of "Archiver" ensures that the processes run as
       user glast (pipeline site daemon SLAC).
  -->
  <task name="Archiver" type="Archiver" version="2.0">
    <notation>
      Replacement for the old Disk Archiver workflow service. Backs up Level 0 data for the day
      7 days before a given base date and fcopy data for the day before the base date.
      </notation>

      <!--
      Dates and times are always UTC since that's what's used for the
      directory hierarchies in the ISOC data archives.      

      Given a base date, usually the date on which the stream was
      created, archive the raw level0 data for the day seven days
      before the base date and archive the fcopy archive for the day
      before the base date. The base date must be supplied in the
      variable "baseDate" defined in the createStream command.

      The archives created are tar files on /nfs/astore-new/g/glstore
      which is a POSIX filesystem interface to HPSS. For compatibility
      with the old Disk Archiver the names of the tar files will have
      the same format, for example:

      FOS-fcopy-2008-11-335.11.30.Sun-0.tar
      FOS-level0-src0077-2008-10-280.10.06.Mon-0.tar

      "FOS" is the "module", used to designate flight-operations data
      as opposed to, say, scientific analysis data or software. The
      old Disk Archiver was a Fermi-wide facility. "src0077" means
      that the source ID, or spacecraft ID, is 77 which is the ID of
      the actual LAT in orbit. The date components are year, month,
      day of year, month again, day of month, and day of week,
      obtained using the "date" command with formats %Y, %m, %j, %m,
      %d and %a respectively. The final number is a tar file sequence
      number. The original astore service had a 10 GB limit on the
      size of data files so the Disk Archiver used the "multi-tape"
      feature of the "tar" command together with an Expect script to
      change the tar file name when the tar command asked for a new
      "tape". This pipeline task always creates a single tar file for
      each archive it makes, so the sequence number will always be
      zero.

      The tar files created by the old Disk Archiver had all file
      paths relative to the ISOC archive root, so this task will do
      the same.

      When it finished a backup the Disk Archiver created a file named
      "archive_done" in the directory that was backed up. This task
      does the same but it doesn't put anything in the file.  This
      marker file must be present along with the file xrootd_done in
      order for a level0 directory to be removed from disk when it's
      more than 60 days old.
   -->

    <variables>
      <var name="taskBase">/sdf/group/fermi/ground/PipelineConfig/Archiver</var>
      <var name="archiveRoot">/sdf/data/fermi/ground/ISOC/flight/Archive</var>
      <var name="astoreRoot">/local-astore/g/glstores</var>
      <var name="isocPlatform">rhel6_gcc44</var>
      <var name="isocFlavor">PROD</var>
      <!-- We are on slurm on S3DF -->
      <var name="jobsite">S3DFDATA</var>
      <var name="extra">--partition=fermi-transfer --account=fermi:other-pipelines --qos=normal --nodes=1 -n1 --cpus-per-task=1 --mem=1G --pty</var>

      <!-- container and wrapper -->
      <var name="container_image">/sdf/group/fermi/sw/containers/fermi-rhel6.sif</var>
      <var name="container_env"></var>
      <var name="container_volumes">--bind /sdf/group/fermi/n:/nfs/farm/g/glast --bind /sdf/group/fermi/a:/afs/slac.stanford.edu/g/glast --bind /sdf/group/fermi/a:/afs/slac/g/glast --bind /sdf:/sdf --bind /lscratch:/lscratch --bind /sdf/group/fermi/sw/package:/afs/slac/package --bind /sdf/group/fermi/sw/package:/afs/slac.stanford.edu/package --bind /sdf/group/fermi/sw/containers/rhel6/opt/TWWfsw:/opt/TWWfsw --bind /sdf/group/fermi/sw/containers/rhel6/usr/local:/usr/local --bind /sdf/group/fermi/sw/java/jdk/jdk8:/usr/lib/jvm/jre-1.8.0-openjdk</var>
      <var name="container_wrap">singularity exec ${container_env} ${container_volumes} ${container_image}</var>
      <var name="isocenv_wrap">sh ${taskBase}/scripts/isocenv_wrapper.sh</var>
    </variables>

    <prerequisites>
      <!-- A UTC date D in ISO format YYYY-MM-DD. Each stream for this
           task will archive the fcopy archive for "D 1 day ago" and
           the level0 date for "D 7 days ago".
      -->
      <prerequisite name="baseDate" type="string"/>
    </prerequisites>

    <process name="fcopy" site="${jobsite}">
      <notation>Archive one day's fcopy archive.</notation>
      <job 
        batchOptions=" --time 00:01:00 ${extra} "
        executable="${taskBase}/scripts/archive_fcopy.sh ${baseDate}"/>
    </process>



    <process name="fcopyCleaner" site="${jobsite}">

      <notation>Run FcopyCleaner.py for the fcopy day that was just backed up,
           but ONLY if the "fcopy" process was successful.
      </notation>

      <job 
        batchOptions=" --time 00:01:00 ${extra} "
        executable="${container_wrap} ${isocenv_wrap} ${taskBase}/scripts/fcopy_cleaner_wrapper.sh ${baseDate}"/>

      <depends>
      	<after process="fcopy" status="SUCCESS" />
      </depends>
    </process>

    <process name="level0" site="${jobsite}">
      <notation>Archive one day of level0 raw data.</notation>
      <job 
        batchOptions=" --time 00:01:00 ${extra} "
        executable="${container_wrap} ${isocenv_wrap} ${taskBase}/scripts/archive_level0.sh ${baseDate}"/>
    </process>



  </task>

</pipeline>